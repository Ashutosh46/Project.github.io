{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project name- Predicting Customer Lifetime value for an auto insurance company using Supervised Machine Learning\n",
    "# Dataset can be downloaded from the below link\n",
    "https://www.kaggle.com/datasets/somjee/auto-insurance-customerlifetimevalue\n",
    "# The main aim of this project is to predict the Cutsomer Lifetime value based on various features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this project we do the following steps\n",
    " 1. Finding missing Values\n",
    " 2. Listing out the Numerical Variables\n",
    " 3. Distribution of the Numerical Variables\n",
    " 4. Categorical Variables\n",
    " 5. Cardinality of Categorical Variables\n",
    " 6. Encoding\n",
    " 7. Splitting the data.\n",
    " 8. Scaling.\n",
    " 9. Modelling the data\n",
    " 10. Testing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we import the necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "## to display all the columns of the dataset \n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "## we import our dataset\n",
    "dataset=pd.read_csv(\"F:\\DATA SETS\\CustomerlifetimeValue-copy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we check the shape of the data,i.e, the number of rows and columns\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to check the dimension of the data\n",
    "dataset.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to print the top 5 records of the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to check whether the data has any null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we find the number of numerical features present in the dataset\n",
    "numerical_features=[feature for feature in dataset.columns if dataset[feature].dtypes!=\"O\"]\n",
    "print(\"count of numerical features:\",len(numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we see the content of the numerical features\n",
    "dataset[numerical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we create a separate variable for the temporal variable\n",
    "temp_var=dataset[\"Effective To Date\"]\n",
    "temp_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We try to find if there is any relationship between the temporal variable and the target variable\n",
    "## We observe that our target variable(CLV) have gone through cyclical fluctuations \n",
    "dataset.groupby(\"Effective To Date\")[\"Customer Lifetime Value\"].median().plot()\n",
    "plt.xlabel(\"Effective To Date\")\n",
    "plt.ylabel(\"median customer lifetime value\")\n",
    "plt.title(\"date vs clv\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We find the number of discrete numerical features in the dataset\n",
    "discrete_features=[feature for feature in numerical_features if len(dataset[feature].unique())<20]\n",
    "print(\"count of discrete variables :\",len(discrete_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the name of the discrete features\n",
    "print(discrete_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the data in the discrete features \n",
    "dataset[discrete_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We plot a barplot to analyze the relationship between the list of discrete features and median_CLV\n",
    "for feature in discrete_features:\n",
    "    dataset.groupby(feature)[\"Customer Lifetime Value\"].median().plot.bar()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"median CLV\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We find the number of continous numerical features in the dataset\n",
    "continous_features=[feature for feature in numerical_features if feature not in discrete_features]\n",
    "print(\"count of continous features:\",len(continous_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the name of the continous features\n",
    "print(continous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the data in continous features\n",
    "dataset[continous_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We plot histograms to find the distribution of the continous features\n",
    "for feature in continous_features:\n",
    "    plt.hist(dataset[feature],bins=20)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We observe that the continous features are skewed so we apply logarithmic function to transform them\n",
    "## only those features are transformed who does not have 0 as a value \n",
    "for feature in continous_features:\n",
    "    if 0 in dataset[feature].unique():\n",
    "        pass\n",
    "    else:\n",
    "        dataset[feature]=np.log(dataset[feature])\n",
    "        plt.hist(dataset[feature],bins=20)\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.title(feature)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We make boxplots to find out the presence of outliers in the continous features\n",
    "for feature in continous_features:\n",
    "    sns.boxplot(y=dataset[feature])\n",
    "    plt.xlabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We find the list of categorical features present in the dataset\n",
    "categorical_features=[feature for feature in dataset.columns if dataset[feature].dtypes==\"O\"]\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove the \"Customer\" column as it is irrelevant\n",
    "categorical_features.remove(\"Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove \"Effective To Date\" column as it is irrelevant to treat it as a categorical feature\n",
    "categorical_features.remove(\"Effective To Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we print the updated list of categorical features\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We find the number of categorical features present in the dataset\n",
    "len(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the data in the categorical features\n",
    "dataset[categorical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We find the cardinality of the categorical features,i.e, the number of sub-categories present in each categorical feature\n",
    "for feature in categorical_features:\n",
    "    print(\"feature is {} and number of sub-categories are {}\".format(feature,len(dataset[feature].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We plot barplot to observe the relationship between the categorical features and the median of the target variable(CLV)\n",
    "for feature in categorical_features:\n",
    "    dataset.groupby(feature)[\"Customer Lifetime Value\"].median().plot.bar()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"median Customer Lifetime Value\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We try to find the percentage of missing values present in the categorical features\n",
    "categorical_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes==\"O\"]\n",
    "for feature in categorical_nan:\n",
    "    print(\"{} has {} % missing values\".format(feature,np.round(dataset[feature].isnull().mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We fill the nan values in the categorical features with a label named \"missing\"\n",
    "dataset[categorical_nan]=dataset[categorical_nan].fillna(\"missing\")\n",
    "## after replacing the nan values we check the whether there is any nan value present\n",
    "dataset[categorical_nan].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the data of the categorical features which had nan values earlier\n",
    "dataset[categorical_nan].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We try to find the percentage of missing values present in the numerical features\n",
    "numerical_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes!=\"O\"]\n",
    "for feature in numerical_nan:\n",
    "    print(\"{} has {} % missing values\".format(feature,np.round(dataset[feature].isnull().mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as we have observed outliers in the data points of numercial features so we replace the nan values with it's median value\n",
    "for feature in numerical_nan:\n",
    "    median_value=dataset[feature].median()\n",
    "    dataset[feature].fillna(median_value,inplace=True)\n",
    "dataset[numerical_nan].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the dataset to find out check whether the nan values have been by median or not\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We replace the sub categories which are present in less than 10% of the dataset with \"rare_var\"label \n",
    "for feature in categorical_features:\n",
    "    temp=dataset.groupby(feature)[\"Customer Lifetime Value\"].count()/len(dataset)\n",
    "    temp_df=temp[temp>0.01].index\n",
    "    dataset[feature]=np.where(dataset[feature].isin(temp_df),dataset[feature],\"rare_var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the dataset to observe the change\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We drop the unnecessary features from the dataset\n",
    "dataset=dataset.drop([\"Customer\",\"Effective To Date\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We perform one hot encoding on categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "columns_to_one_hot = ['State','Response','Coverage','Education','EmploymentStatus','Gender','Location Code','Marital Status','Policy Type','Policy','Renew Offer Type','Sales Channel','Vehicle Class','Vehicle Size']\n",
    "encoded_array = enc.fit_transform(dataset.loc[:,columns_to_one_hot])\n",
    "dataset_encoded = pd.DataFrame(encoded_array,columns=enc.get_feature_names_out() )\n",
    "dataset_sklearn_encoded = pd.concat([dataset,dataset_encoded],axis=1)\n",
    "dataset_sklearn_encoded.drop(labels= columns_to_one_hot,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the encoded dataset\n",
    "dataset_sklearn_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We separate the independent and dependent variables from the dataset\n",
    "x=dataset_sklearn_encoded.drop(\"Customer Lifetime Value\",axis=1)\n",
    "y=dataset_sklearn_encoded[\"Customer Lifetime Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the independent varibale\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the dependent variable\n",
    "y=pd.DataFrame(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We split our data into train and test data \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use Standardization method to scale down all the features in the dataset \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "x_train=scaler.fit_transform(x_train)\n",
    "x_test=scaler.transform(x_test)\n",
    "y_train=scaler.fit_transform(y_train)\n",
    "y_test=scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the transformed x_train\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the transformed y_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We import certain modules from the sklearn library\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We fit our dataset into lasso regression to select relevant features of the dataset to be considered for modelling\n",
    "feature_sel_model=SelectFromModel(Lasso(alpha=0.005,random_state=0))\n",
    "feature_sel_model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print an array indicating which features are selected\n",
    "feature_sel_model.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We count the number of selected features  \n",
    "selected_feat=x.columns[(feature_sel_model.get_support())]\n",
    "print(\"number of features selected: {}\".format(len(selected_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We create a list of the selected features and print them\n",
    "selected_feat=list(selected_feat)\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We create a list of independent variables and build the x_train dataframe with it\n",
    "feature_scale=[feature for feature in dataset_sklearn_encoded.columns if feature not in [\"Customer Lifetime Value\"]]\n",
    "x_train=pd.DataFrame(x_train,columns=feature_scale)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=pd.DataFrame(x_test,columns=feature_scale)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we reduce the dimension of x_train data by considering only the selected features\n",
    "x_train=x_train[['Monthly Premium Auto', 'Months Since Last Claim', 'Months Since Policy Inception', 'Number of Open Complaints', 'Number of Policies', 'Total Claim Amount', 'EmploymentStatus_Employed', 'Marital Status_Single', 'Renew Offer Type_Offer1', 'Renew Offer Type_Offer2', 'Vehicle Class_Four-Door Car', 'Vehicle Class_SUV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we reduce the dimension of x_test data by considering only the selected features\n",
    "x_test=x_test[['Monthly Premium Auto', 'Months Since Last Claim', 'Months Since Policy Inception', 'Number of Open Complaints', 'Number of Policies', 'Total Claim Amount', 'EmploymentStatus_Employed', 'Marital Status_Single', 'Renew Offer Type_Offer1', 'Renew Offer Type_Offer2', 'Vehicle Class_Four-Door Car', 'Vehicle Class_SUV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We convert the x_train dataframe into an array\n",
    "x_train=np.array(x_train)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We convert the x_test dataframe into an array\n",
    "x_test=np.array(x_test)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We check the shape of the x_train data\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We check the shape of the x_test data\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We check the shape of the y_train data\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We check the shape of the y_test data\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor=RandomForestRegressor(n_estimators = 500,random_state=6)\n",
    "regressor.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We predict using the x_test data\n",
    "y_pred=regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the predicted values\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.predict([x_test[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.predict([[100,3,12,0,1,1200,1,1,1,1,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We construct a dataframe to display the actual and predicted values together\n",
    "df=pd.DataFrame({\"actual\":y_test.ravel(),\"predicted\":y_pred.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## We use score to see how well the data is performing for train data\n",
    "regressor.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use score to see how well the data is performing for test data\n",
    "regressor.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We check the r2_score \n",
    "metrics.r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x[['Monthly Premium Auto', 'Months Since Last Claim', 'Months Since Policy Inception', 'Number of Open Complaints', 'Number of Policies', 'Total Claim Amount', 'EmploymentStatus_Employed', 'Marital Status_Single', 'Renew Offer Type_Offer1', 'Renew Offer Type_Offer2', 'Vehicle Class_Four-Door Car', 'Vehicle Class_SUV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sizes=[1,5,10,15,20,25,30,50,100,150,300]\n",
    "## sizes,training_scores,testing_scores=learning_curve(regressor,x,y,cv=5,scoring='neg_mean_squared_error',train_sizes=sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_training=np.mean(training_scores,axis=1)\n",
    "# mean_testing=np.mean(testing_scores,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(sizes,mean_training,color='r',linestyle='--',label='Training score')\n",
    "# plt.plot(sizes,mean_testing,color='y',label='Testing score')\n",
    "# plt.title('Learning curve for the regressor model',fontsize=20)\n",
    "# plt.xlabel('Training set size',fontsize=15)\n",
    "# plt.ylabel('mean squared error',fontsize=15)\n",
    "# plt.legend(loc='best',fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(regressor,open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pickle.load(open('model.pkl','rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
